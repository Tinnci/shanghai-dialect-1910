#!/usr/bin/env python3
"""
ç”Ÿæˆå¼‚å¸¸å‘éŸ³çš„æ±‡æ€»æŠ¥å‘Šï¼ˆæŒ‰æ–‡ä»¶åˆ†ç»„ï¼‰
"""

import re
import json
from pathlib import Path
from collections import defaultdict

def extract_ruby_pairs(content: str, filename: str) -> list:
    pattern = r'#r\("([^"]+)",\s*"([^"]+)"\)'
    matches = re.findall(pattern, content)
    return [(pinyin, hanzi, filename) for pinyin, hanzi in matches]

def normalize_pinyin(pinyin: str) -> str:
    return re.sub(r'[,.\?!;:ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]+$', '', pinyin).strip().lower()

def split_characters(hanzi: str) -> list[str]:
    chars = []
    for char in hanzi:
        if '\u4e00' <= char <= '\u9fff':
            chars.append(char)
    return chars

def main():
    lessons_dir = Path("/home/drie/ä¸‹è½½/Shanghai Dialect Exercises in Romanized and Character with Key to Pronunciation and English Index/typst_source/contents/lessons")
    
    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰å‘éŸ³ç»Ÿè®¡
    char_pronunciations = defaultdict(lambda: defaultdict(list))
    
    for lesson_file in sorted(lessons_dir.glob("lesson-*.typ")):
        content = lesson_file.read_text(encoding='utf-8')
        pairs = extract_ruby_pairs(content, lesson_file.name)
        
        for pinyin, hanzi, filename in pairs:
            chars = split_characters(hanzi)
            pinyin_parts = re.split(r'[-\s]', normalize_pinyin(pinyin))
            
            if len(pinyin_parts) == len(chars):
                for char, py in zip(chars, pinyin_parts):
                    char_pronunciations[char][py].append((filename, hanzi, pinyin))
            elif len(chars) == 1:
                char_pronunciations[chars[0]][normalize_pinyin(pinyin)].append((filename, hanzi, pinyin))
    
    # ç¡®å®šæ¯ä¸ªå­—çš„ä¸»è¦å‘éŸ³
    main_pronunciations = {}
    for char, pronunciations in char_pronunciations.items():
        if len(pronunciations) >= 1:
            main_py = max(pronunciations.keys(), key=lambda x: len(pronunciations[x]))
            main_pronunciations[char] = (main_py, len(pronunciations[main_py]))
    
    # ç¬¬äºŒéï¼šæ‰¾å‡ºæ¯ä¸ªæ–‡ä»¶ä¸­çš„å¼‚å¸¸
    file_anomalies = defaultdict(list)
    
    for lesson_file in sorted(lessons_dir.glob("lesson-*.typ")):
        content = lesson_file.read_text(encoding='utf-8')
        pairs = extract_ruby_pairs(content, lesson_file.name)
        
        for pinyin, hanzi, filename in pairs:
            chars = split_characters(hanzi)
            pinyin_parts = re.split(r'[-\s]', normalize_pinyin(pinyin))
            
            char_py_pairs = []
            if len(pinyin_parts) == len(chars):
                char_py_pairs = list(zip(chars, pinyin_parts))
            elif len(chars) == 1:
                char_py_pairs = [(chars[0], normalize_pinyin(pinyin))]
            
            for char, py in char_py_pairs:
                if char in main_pronunciations:
                    main_py, main_count = main_pronunciations[char]
                    total = sum(len(occ) for occ in char_pronunciations[char].values())
                    py_count = len(char_pronunciations[char].get(py, []))
                    
                    # å¦‚æœä¸»è¦å‘éŸ³å æ¯”>80%ä¸”å‡ºç°>=3æ¬¡ï¼Œä¸”å½“å‰å‘éŸ³ä¸æ˜¯ä¸»è¦å‘éŸ³
                    if main_count / total >= 0.8 and total >= 3 and py != main_py:
                        file_anomalies[filename].append({
                            'char': char,
                            'found': py,
                            'expected': main_py,
                            'main_count': main_count,
                            'found_count': py_count,
                            'original_hanzi': hanzi,
                            'original_pinyin': pinyin
                        })
    
    # è¾“å‡ºæŠ¥å‘Š
    print("=" * 100)
    print("ä¸Šæµ·è¯ç»ƒä¹ å†Œ - æŒ‰æ–‡ä»¶åˆ†ç»„çš„å¼‚å¸¸å‘éŸ³æŠ¥å‘Š")
    print("=" * 100)
    print()
    
    # æŒ‰å¼‚å¸¸æ•°é‡æ’åºæ–‡ä»¶
    sorted_files = sorted(file_anomalies.items(), key=lambda x: -len(x[1]))
    
    total_anomalies = sum(len(v) for v in file_anomalies.values())
    print(f"æ€»è®¡å‘ç° {total_anomalies} å¤„é«˜ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œåˆ†å¸ƒåœ¨ {len(file_anomalies)} ä¸ªæ–‡ä»¶ä¸­")
    print()
    
    # é«˜å¼‚å¸¸ç‡æ–‡ä»¶ï¼ˆå¯èƒ½éœ€è¦é‡ç‚¹å…³æ³¨ï¼‰
    print("### å¼‚å¸¸æ•°é‡æœ€å¤šçš„æ–‡ä»¶ï¼ˆTop 20ï¼‰")
    print("-" * 100)
    for filename, anomalies in sorted_files[:20]:
        print(f"\nğŸ“ **{filename}** - {len(anomalies)} å¤„å¼‚å¸¸")
        # æŒ‰å­—ç¬¦åˆ†ç»„
        by_char = defaultdict(list)
        for a in anomalies:
            by_char[a['char']].append(a)
        
        for char, items in sorted(by_char.items(), key=lambda x: -len(x[1])):
            first = items[0]
            print(f"  â€¢ ã€Œ{char}ã€ æœŸæœ›: {first['expected']}({first['main_count']}æ¬¡) â†’ å‘ç°: {first['found']}({first['found_count']}æ¬¡)")
            for item in items[:3]:
                print(f"      #r(\"{item['original_pinyin']}\", \"{item['original_hanzi']}\")")
            if len(items) > 3:
                print(f"      ... è¿˜æœ‰ {len(items)-3} å¤„")
    
    # ç”Ÿæˆå¯æ“ä½œçš„ä¿®å¤æ¸…å•
    print()
    print("=" * 100)
    print("ä¿®å¤å»ºè®®æ¸…å•ï¼ˆæŒ‰æ–‡ä»¶ï¼‰")
    print("=" * 100)
    
    for filename, anomalies in sorted_files:
        if len(anomalies) == 0:
            continue
        print(f"\n## {filename}")
        seen = set()
        for a in anomalies:
            key = (a['original_pinyin'], a['original_hanzi'])
            if key not in seen:
                seen.add(key)
                print(f"  #r(\"{a['original_pinyin']}\", \"{a['original_hanzi']}\")")
                print(f"    â†’ ã€Œ{a['char']}ã€åº”ä¸º {a['expected']} (ä¸»æµå‘éŸ³ï¼Œ{a['main_count']}æ¬¡)")

if __name__ == "__main__":
    main()
