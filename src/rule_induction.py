"""
éŸ³éŸµè§„åˆ™æ¨å¯¼æ¨¡å— (Phonological Rule Induction)

å®ç°æ™ºèƒ½åŒ–çš„å¯å‘å¼åˆ†æç®—æ³•ï¼š
1. ä»å¹³è¡Œè¯­æ–™(æœ¬ä¹¦ vs Rimeè¯å…¸)è‡ªåŠ¨æ¨å¯¼è½¬æ¢è§„åˆ™
2. åŸºäºéŸ³éŸµç‰¹å¾çš„åŠ æƒç¼–è¾‘è·ç¦»
3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šéŸ³å­—æ¶ˆæ­§
4. è§„åˆ™ç½®ä¿¡åº¦è¯„ä¼°

ç®—æ³•åˆ†ç±»:
- Rule Induction (è§„åˆ™æ¨å¯¼): ä»æ•°æ®ä¸­è‡ªåŠ¨å‘ç°æ¨¡å¼
- Weighted Edit Distance (åŠ æƒç¼–è¾‘è·ç¦»): éŸ³éŸµå­¦è·ç¦»è€Œéå­—ç¬¦è·ç¦»
- Feature-based Similarity (ç‰¹å¾ç›¸ä¼¼åº¦): åŸºäºå£°éŸµè°ƒç‰¹å¾å‘é‡
- Context-aware Disambiguation (ä¸Šä¸‹æ–‡æ¶ˆæ­§): åˆ©ç”¨è¯ç»„ä¸Šä¸‹æ–‡
"""

from collections import defaultdict, Counter
from dataclasses import dataclass, field
from typing import Dict, List, Tuple

# ============================================================================
# éŸ³éŸµç‰¹å¾ç³»ç»Ÿ (Phonological Feature System)
# ============================================================================


@dataclass
class PhoneticFeatures:
    """éŸ³éŸµç‰¹å¾å‘é‡"""

    # å£°æ¯ç‰¹å¾
    place: str = ""  # å‘éŸ³éƒ¨ä½: labial, dental, velar, palatal, glottal
    manner: str = ""  # å‘éŸ³æ–¹å¼: stop, fricative, affricate, nasal, lateral
    voiced: bool = False  # æµŠéŸ³
    aspirated: bool = False  # é€æ°”

    # éŸµæ¯ç‰¹å¾
    vowel_height: str = ""  # å…ƒéŸ³é«˜åº¦: high, mid, low
    vowel_front: str = ""  # å…ƒéŸ³å‰å: front, central, back
    vowel_round: bool = False  # åœ†å”‡
    nasal_coda: bool = False  # é¼»éŸµå°¾
    stop_coda: bool = False  # å…¥å£°éŸµå°¾ (å–‰å¡)

    def to_vector(self) -> List[float]:
        """è½¬æ¢ä¸ºæ•°å€¼å‘é‡ï¼Œç”¨äºè®¡ç®—è·ç¦»"""
        place_map = {
            "labial": 0,
            "dental": 1,
            "velar": 2,
            "palatal": 3,
            "glottal": 4,
            "": -1,
        }
        manner_map = {
            "stop": 0,
            "fricative": 1,
            "affricate": 2,
            "nasal": 3,
            "lateral": 4,
            "approximant": 5,
            "": -1,
        }
        height_map = {"high": 0, "mid": 1, "low": 2, "": -1}
        front_map = {"front": 0, "central": 1, "back": 2, "": -1}

        return [
            place_map.get(self.place, -1),
            manner_map.get(self.manner, -1),
            float(self.voiced),
            float(self.aspirated),
            height_map.get(self.vowel_height, -1),
            front_map.get(self.vowel_front, -1),
            float(self.vowel_round),
            float(self.nasal_coda),
            float(self.stop_coda),
        ]


# å£°æ¯ç‰¹å¾åº“
INITIAL_FEATURES = {
    # å”‡éŸ³
    "p": PhoneticFeatures(place="labial", manner="stop", voiced=False, aspirated=False),
    "ph": PhoneticFeatures(place="labial", manner="stop", voiced=False, aspirated=True),
    "b": PhoneticFeatures(place="labial", manner="stop", voiced=True, aspirated=False),
    "m": PhoneticFeatures(place="labial", manner="nasal", voiced=True),
    "f": PhoneticFeatures(place="labial", manner="fricative", voiced=False),
    "v": PhoneticFeatures(place="labial", manner="fricative", voiced=True),
    "w": PhoneticFeatures(place="labial", manner="approximant", voiced=True),
    # é½¿éŸ³
    "t": PhoneticFeatures(place="dental", manner="stop", voiced=False, aspirated=False),
    "th": PhoneticFeatures(place="dental", manner="stop", voiced=False, aspirated=True),
    "d": PhoneticFeatures(place="dental", manner="stop", voiced=True, aspirated=False),
    "n": PhoneticFeatures(place="dental", manner="nasal", voiced=True),
    "l": PhoneticFeatures(place="dental", manner="lateral", voiced=True),
    "lh": PhoneticFeatures(place="dental", manner="lateral", voiced=False),  # æ¸…è¾¹éŸ³
    # é½¿é¾ˆå¡æ“¦/æ“¦éŸ³
    "ts": PhoneticFeatures(
        place="dental", manner="affricate", voiced=False, aspirated=False
    ),
    "tsh": PhoneticFeatures(
        place="dental", manner="affricate", voiced=False, aspirated=True
    ),
    "c": PhoneticFeatures(
        place="dental", manner="affricate", voiced=False, aspirated=False
    ),
    "ch": PhoneticFeatures(
        place="dental", manner="affricate", voiced=False, aspirated=True
    ),
    "dz": PhoneticFeatures(
        place="dental", manner="affricate", voiced=True, aspirated=False
    ),
    "j": PhoneticFeatures(
        place="dental", manner="affricate", voiced=True, aspirated=False
    ),
    "s": PhoneticFeatures(place="dental", manner="fricative", voiced=False),
    "z": PhoneticFeatures(place="dental", manner="fricative", voiced=True),
    "sh": PhoneticFeatures(place="dental", manner="fricative", voiced=False),
    "zh": PhoneticFeatures(place="dental", manner="fricative", voiced=True),
    # è…­éŸ³
    "ny": PhoneticFeatures(place="palatal", manner="nasal", voiced=True),
    "gn": PhoneticFeatures(place="palatal", manner="nasal", voiced=True),
    "ky": PhoneticFeatures(
        place="palatal", manner="stop", voiced=False, aspirated=False
    ),
    "hy": PhoneticFeatures(place="palatal", manner="fricative", voiced=False),
    "x": PhoneticFeatures(place="palatal", manner="fricative", voiced=False),
    "y": PhoneticFeatures(place="palatal", manner="approximant", voiced=True),
    # è½¯è…­éŸ³
    "k": PhoneticFeatures(place="velar", manner="stop", voiced=False, aspirated=False),
    "kh": PhoneticFeatures(place="velar", manner="stop", voiced=False, aspirated=True),
    "g": PhoneticFeatures(place="velar", manner="stop", voiced=True, aspirated=False),
    "gh": PhoneticFeatures(place="velar", manner="fricative", voiced=True),
    "ng": PhoneticFeatures(place="velar", manner="nasal", voiced=True),
    # å–‰éŸ³
    "h": PhoneticFeatures(place="glottal", manner="fricative", voiced=False),
    "'": PhoneticFeatures(place="glottal", manner="stop", voiced=False),
    "hh": PhoneticFeatures(place="glottal", manner="stop", voiced=False),
    # é›¶å£°æ¯
    "": PhoneticFeatures(),
}

# éŸµæ¯ç‰¹å¾åº“ (ç®€åŒ–ç‰ˆ)
FINAL_FEATURES = {
    # å¼€å£å‘¼
    "a": PhoneticFeatures(vowel_height="low", vowel_front="central"),
    "e": PhoneticFeatures(vowel_height="mid", vowel_front="front"),
    "o": PhoneticFeatures(vowel_height="mid", vowel_front="back", vowel_round=True),
    "i": PhoneticFeatures(vowel_height="high", vowel_front="front"),
    "u": PhoneticFeatures(vowel_height="high", vowel_front="back", vowel_round=True),
    "oe": PhoneticFeatures(vowel_height="mid", vowel_front="front", vowel_round=True),
    "eu": PhoneticFeatures(vowel_height="mid", vowel_front="front", vowel_round=True),
    # å…¥å£°
    "aq": PhoneticFeatures(vowel_height="low", vowel_front="central", stop_coda=True),
    "eq": PhoneticFeatures(vowel_height="mid", vowel_front="front", stop_coda=True),
    "iq": PhoneticFeatures(vowel_height="high", vowel_front="front", stop_coda=True),
    "oq": PhoneticFeatures(
        vowel_height="mid", vowel_front="back", vowel_round=True, stop_coda=True
    ),
    "uq": PhoneticFeatures(
        vowel_height="high", vowel_front="back", vowel_round=True, stop_coda=True
    ),
    "ah": PhoneticFeatures(vowel_height="low", vowel_front="central", stop_coda=True),
    "eh": PhoneticFeatures(vowel_height="mid", vowel_front="front", stop_coda=True),
    "ih": PhoneticFeatures(vowel_height="high", vowel_front="front", stop_coda=True),
    "oh": PhoneticFeatures(
        vowel_height="mid", vowel_front="back", vowel_round=True, stop_coda=True
    ),
    "uh": PhoneticFeatures(
        vowel_height="high", vowel_front="back", vowel_round=True, stop_coda=True
    ),
    # é¼»éŸµ
    "an": PhoneticFeatures(vowel_height="low", vowel_front="central", nasal_coda=True),
    "en": PhoneticFeatures(vowel_height="mid", vowel_front="front", nasal_coda=True),
    "in": PhoneticFeatures(vowel_height="high", vowel_front="front", nasal_coda=True),
    "on": PhoneticFeatures(
        vowel_height="mid", vowel_front="back", vowel_round=True, nasal_coda=True
    ),
    "aon": PhoneticFeatures(vowel_height="low", vowel_front="back", nasal_coda=True),
    "aung": PhoneticFeatures(vowel_height="low", vowel_front="back", nasal_coda=True),
}


def feature_distance(f1: PhoneticFeatures, f2: PhoneticFeatures) -> float:
    """è®¡ç®—ä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„è·ç¦» (0-1)"""
    v1, v2 = f1.to_vector(), f2.to_vector()

    # å¿½ç•¥æœªå®šä¹‰çš„ç‰¹å¾ (-1)
    diff_sum = 0
    count = 0
    for a, b in zip(v1, v2):
        if a >= 0 and b >= 0:
            diff_sum += abs(a - b)
            count += 1

    if count == 0:
        return 1.0

    # å½’ä¸€åŒ–åˆ° 0-1
    return min(1.0, diff_sum / (count * 2))


def phonetic_feature_similarity(
    init1: str, final1: str, init2: str, final2: str
) -> float:
    """
    åŸºäºéŸ³éŸµç‰¹å¾è®¡ç®—ä¸¤ä¸ªéŸ³èŠ‚çš„ç›¸ä¼¼åº¦

    è¿”å› 0-1 ä¹‹é—´çš„ç›¸ä¼¼åº¦
    """
    # è·å–ç‰¹å¾
    f_init1 = INITIAL_FEATURES.get(init1.lower(), PhoneticFeatures())
    f_init2 = INITIAL_FEATURES.get(init2.lower(), PhoneticFeatures())
    f_final1 = FINAL_FEATURES.get(final1.lower(), PhoneticFeatures())
    f_final2 = FINAL_FEATURES.get(final2.lower(), PhoneticFeatures())

    # è®¡ç®—è·ç¦»
    init_dist = feature_distance(f_init1, f_init2)
    final_dist = feature_distance(f_final1, f_final2)

    # åŠ æƒå¹³å‡ (å£°æ¯æƒé‡ç¨é«˜)
    total_dist = init_dist * 0.6 + final_dist * 0.4

    return 1.0 - total_dist


# ============================================================================
# è§„åˆ™æ¨å¯¼å¼•æ“ (Rule Induction Engine)
# ============================================================================


@dataclass
class TransformRule:
    """è½¬æ¢è§„åˆ™"""

    source: str  # æºæ¨¡å¼ (æ•™ä¼šç½—é©¬å­—)
    target: str  # ç›®æ ‡æ¨¡å¼ (å´è¯­å­¦å ‚)
    rule_type: str  # è§„åˆ™ç±»å‹: initial, final, tone
    context: str = ""  # ä¸Šä¸‹æ–‡æ¡ä»¶ (å¯é€‰)
    count: int = 0  # å‡ºç°æ¬¡æ•°
    examples: List[str] = field(default_factory=list)  # ç¤ºä¾‹

    @property
    def confidence(self) -> float:
        """åŸºäºå‡ºç°æ¬¡æ•°çš„ç½®ä¿¡åº¦"""
        return min(1.0, self.count / 10)  # 10æ¬¡ä»¥ä¸Šä¸ºé«˜ç½®ä¿¡åº¦


class RuleInductionEngine:
    """
    è§„åˆ™æ¨å¯¼å¼•æ“

    ä»å¹³è¡Œè¯­æ–™ä¸­è‡ªåŠ¨å‘ç°è½¬æ¢è§„åˆ™
    """

    def __init__(self):
        self.initial_rules: Dict[str, List[TransformRule]] = defaultdict(list)
        self.final_rules: Dict[str, List[TransformRule]] = defaultdict(list)
        self.learned_pairs: List[Tuple[str, str, str]] = []  # (church, wugniu, hanzi)

    def add_parallel_pair(
        self, church_pinyin: str, wugniu_pinyin: str, hanzi: str = ""
    ):
        """æ·»åŠ ä¸€å¯¹å¹³è¡Œæ‹¼éŸ³ç”¨äºå­¦ä¹ """
        self.learned_pairs.append((church_pinyin.lower(), wugniu_pinyin.lower(), hanzi))

    def induce_rules(self) -> Tuple[List[TransformRule], List[TransformRule]]:
        """
        ä»å¹³è¡Œè¯­æ–™ä¸­æ¨å¯¼è§„åˆ™

        ä½¿ç”¨ æœ€å°ç¼–è¾‘è„šæœ¬ (Minimum Edit Script) ç®—æ³•
        """
        initial_counter = Counter()
        final_counter = Counter()

        for church, wugniu, hanzi in self.learned_pairs:
            # åˆ†ç¦»å£°æ¯éŸµæ¯
            c_init, c_final = self._split_syllable(church, is_wugniu=False)
            w_init, w_final = self._split_syllable(wugniu, is_wugniu=True)

            # è®°å½•å¯¹åº”å…³ç³»
            if c_init or w_init:
                initial_counter[(c_init, w_init)] += 1
            if c_final or w_final:
                final_counter[(c_final, w_final)] += 1

        # ç”Ÿæˆè§„åˆ™
        initial_rules = []
        for (src, tgt), count in initial_counter.most_common():
            if src != tgt:  # åªè®°å½•ä¸åŒçš„
                rule = TransformRule(
                    source=src, target=tgt, rule_type="initial", count=count
                )
                initial_rules.append(rule)

        final_rules = []
        for (src, tgt), count in final_counter.most_common():
            if src != tgt:
                rule = TransformRule(
                    source=src, target=tgt, rule_type="final", count=count
                )
                final_rules.append(rule)

        return initial_rules, final_rules

    def _split_syllable(
        self, syllable: str, is_wugniu: bool = False
    ) -> Tuple[str, str]:
        """åˆ†ç¦»å£°æ¯å’ŒéŸµæ¯"""
        syllable = syllable.lower().strip("',.-")
        if not syllable:
            return "", ""

        # å£°æ¯åˆ—è¡¨ (æŒ‰é•¿åº¦æ’åº)
        if is_wugniu:
            initials = [
                "tsh",
                "dz",
                "ts",
                "ng",
                "gn",
                "kh",
                "ph",
                "th",
                "gh",
                "ch",
                "sh",
                "zh",
                "k",
                "h",
                "m",
                "n",
                "l",
                "v",
                "w",
                "f",
                "p",
                "b",
                "t",
                "d",
                "s",
                "z",
                "j",
                "c",
                "q",
                "x",
                "y",
                "'",
            ]
        else:
            initials = [
                "tsh",
                "dz",
                "ts",
                "ng",
                "ny",
                "kh",
                "ph",
                "th",
                "gh",
                "ch",
                "sh",
                "ky",
                "hy",
                "kw",
                "gw",
                "hw",
                "k",
                "h",
                "m",
                "n",
                "l",
                "v",
                "w",
                "f",
                "p",
                "b",
                "t",
                "d",
                "s",
                "z",
                "j",
                "y",
                "'",
            ]

        for init in sorted(initials, key=lambda x: -len(x)):
            if syllable.startswith(init):
                return init, syllable[len(init) :]

        return "", syllable

    def print_rules_report(self):
        """æ‰“å°è§„åˆ™æŠ¥å‘Š"""
        initial_rules, final_rules = self.induce_rules()

        print("=" * 60)
        print("è‡ªåŠ¨æ¨å¯¼çš„è½¬æ¢è§„åˆ™ (Rule Induction Results)")
        print("=" * 60)

        print("\nğŸ“Œ å£°æ¯è§„åˆ™ (Initial Rules):")
        for rule in initial_rules[:20]:
            conf = (
                "ğŸŸ¢"
                if rule.confidence > 0.8
                else "ğŸŸ¡"
                if rule.confidence > 0.5
                else "ğŸ”´"
            )
            print(f"  {conf} '{rule.source}' â†’ '{rule.target}' (å‡ºç° {rule.count} æ¬¡)")

        print("\nğŸ“Œ éŸµæ¯è§„åˆ™ (Final Rules):")
        for rule in final_rules[:20]:
            conf = (
                "ğŸŸ¢"
                if rule.confidence > 0.8
                else "ğŸŸ¡"
                if rule.confidence > 0.5
                else "ğŸ”´"
            )
            print(f"  {conf} '{rule.source}' â†’ '{rule.target}' (å‡ºç° {rule.count} æ¬¡)")


# ============================================================================
# æ™ºèƒ½åˆ†æå™¨ (Intelligent Analyzer)
# ============================================================================


class IntelligentAnalyzer:
    """
    æ™ºèƒ½åˆ†æå™¨

    ç»¼åˆä½¿ç”¨å¤šç§å¯å‘å¼è§„åˆ™è¿›è¡Œåˆ†æ
    """

    def __init__(self):
        self.rule_engine = RuleInductionEngine()
        self._load_rime_data()

    def _load_rime_data(self):
        """åŠ è½½ Rime è¯å…¸æ•°æ®"""
        try:
            from .rime_dict import get_rime_data

            self.char_pinyins, self.phrase_pinyins, self.polyphonic = get_rime_data()
        except ImportError:
            self.char_pinyins = {}
            self.phrase_pinyins = {}
            self.polyphonic = set()

    def analyze_with_heuristics(self, church_pinyin: str, hanzi: str) -> Dict:
        """
        ä½¿ç”¨å¯å‘å¼è§„åˆ™åˆ†ææ‹¼éŸ³-æ±‰å­—å¯¹

        è¿”å›åˆ†æç»“æœï¼ŒåŒ…æ‹¬:
        - is_valid: æ˜¯å¦åˆæ³•
        - confidence: ç½®ä¿¡åº¦
        - reasons: åˆ¤æ–­åŸå› 
        - suggestions: å»ºè®®
        """
        result = {
            "is_valid": True,
            "confidence": 1.0,
            "reasons": [],
            "suggestions": [],
        }

        # å¯å‘å¼è§„åˆ™1: æ£€æŸ¥æ˜¯å¦ä¸ºå¤šéŸ³å­—
        if hanzi in self.polyphonic:
            result["reasons"].append(f"'{hanzi}' æ˜¯å¤šéŸ³å­—ï¼Œéœ€è¦ä¸Šä¸‹æ–‡åˆ¤æ–­")
            result["confidence"] *= 0.8

        # å¯å‘å¼è§„åˆ™2: æ£€æŸ¥ Rime è¯å…¸ä¸­çš„å˜ä½“
        if hanzi in self.char_pinyins:
            wugniu_variants = self.char_pinyins[hanzi]

            # ä½¿ç”¨ç‰¹å¾ç›¸ä¼¼åº¦æ£€æŸ¥
            best_match = None
            best_sim = 0

            c_init, c_final = self.rule_engine._split_syllable(
                church_pinyin, is_wugniu=False
            )

            for variant in wugniu_variants:
                w_init, w_final = self.rule_engine._split_syllable(
                    variant, is_wugniu=True
                )
                sim = phonetic_feature_similarity(c_init, c_final, w_init, w_final)
                if sim > best_sim:
                    best_sim = sim
                    best_match = variant

            if best_sim > 0.7:
                result["reasons"].append(
                    f"éŸ³éŸµç‰¹å¾åŒ¹é…: '{church_pinyin}' â‰ˆ '{best_match}' (ç›¸ä¼¼åº¦: {best_sim:.2f})"
                )
            else:
                result["is_valid"] = False
                result["confidence"] *= best_sim
                result["reasons"].append(
                    f"æœ€æ¥è¿‘çš„è¯»éŸ³æ˜¯ '{best_match}' (ç›¸ä¼¼åº¦: {best_sim:.2f})"
                )
                result["suggestions"].append(f"å»ºè®®æ£€æŸ¥: {wugniu_variants}")

        # å¯å‘å¼è§„åˆ™3: å…¥å£°éŸµå°¾æ£€æŸ¥
        if church_pinyin.endswith(("h", "k")) and not church_pinyin.endswith(
            ("ng", "ung", "aung")
        ):
            result["reasons"].append("æ£€æµ‹åˆ°å…¥å£°éŸµå°¾ (-h/-k â†’ -q)")

        return result


# ============================================================================
# æµ‹è¯•
# ============================================================================

if __name__ == "__main__":
    # æµ‹è¯•ç‰¹å¾ç›¸ä¼¼åº¦
    print("=" * 60)
    print("éŸ³éŸµç‰¹å¾ç›¸ä¼¼åº¦æµ‹è¯•")
    print("=" * 60)

    test_cases = [
        ("ny", "ih", "gn", "iq"),  # nyih vs gniq (æ—¥)
        ("l", "eh", "l", "eq"),  # leh vs leq (å‹’)
        ("l", "a", "l", "a"),  # la vs la (æ‹‰)
        ("ts", "ang", "c", "an"),  # tsang vs can
        ("dz", "oong", "j", "on"),  # dzoong vs jon
    ]

    for c_init, c_final, w_init, w_final in test_cases:
        sim = phonetic_feature_similarity(c_init, c_final, w_init, w_final)
        print(f"  {c_init}{c_final} vs {w_init}{w_final}: ç›¸ä¼¼åº¦ = {sim:.2f}")

    # æµ‹è¯•è§„åˆ™æ¨å¯¼
    print("\n" + "=" * 60)
    print("è§„åˆ™æ¨å¯¼æµ‹è¯•")
    print("=" * 60)

    engine = RuleInductionEngine()

    # æ·»åŠ ä¸€äº›è®­ç»ƒæ•°æ®
    training_pairs = [
        ("nyih", "gniq", "æ—¥"),
        ("zeh", "zeq", "æ—¥"),
        ("la", "la", "æ‹‰"),
        ("leh", "leq", "å‹’"),
        ("tshang", "chan", "é•¿"),
        ("dzoong", "jon", "ä»"),
        ("aung", "aon", "æ˜‚"),
        ("kuh", "keq", "ä¸ª"),
        ("nyung", "gnin", "äºº"),
        ("tseu", "ceu", "èµ°"),
    ]

    for church, wugniu, hanzi in training_pairs:
        engine.add_parallel_pair(church, wugniu, hanzi)

    engine.print_rules_report()
