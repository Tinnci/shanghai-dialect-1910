"""
Ruby å¯¹è‡ªåŠ¨ä¿®å¤æ¨¡å—
æ”¯æŒäº¤äº’å¼ä¿®å¤ã€å¹²è¿è¡Œå’Œè‡ªåŠ¨æ¨¡å¼
"""
import re
import shutil
from pathlib import Path
from dataclasses import dataclass
from enum import Enum, auto
from typing import List, Optional, Dict
from collections import defaultdict

from .loader import LessonFile, load_lessons
from .utils import split_characters, get_similarity, normalize_pinyin

class FixStrategy(Enum):
    SPLIT_RUBY = auto()      # æ‹†åˆ†ï¼šæ±‰å­—å¤šäºæ‹¼éŸ³
    MERGE_RUBY = auto()      # åˆå¹¶ï¼šæ‹¼éŸ³å¤šäºæ±‰å­—
    REPLACE_PINYIN = auto()  # æ›¿æ¢ï¼šæ‹¼å†™é”™è¯¯
    MANUAL = auto()          # æ— æ³•è‡ªåŠ¨å¤„ç†

@dataclass
class FixSuggestion:
    """å•æ¡ä¿®å¤å»ºè®®"""
    file: str
    line_num: int
    strategy: FixStrategy
    original: str           # åŸå§‹ #r(...) æ–‡æœ¬
    problem: str            # é—®é¢˜æè¿°
    suggestion: str         # å»ºè®®çš„ä¿®å¤åæ–‡æœ¬
    confidence: float       # ç½®ä¿¡åº¦ (0-1)
    needs_input: bool = False  # æ˜¯å¦éœ€è¦ç”¨æˆ·è¾“å…¥
    missing_char: str = ""     # éœ€è¦ç”¨æˆ·æä¾›æ‹¼éŸ³çš„æ±‰å­—

def build_pronunciation_db(lessons: List[LessonFile]) -> Dict[str, str]:
    """ä»å…¨ä¹¦æ„å»ºæ±‰å­—-ä¸»æµè¯»éŸ³æ•°æ®åº“"""
    char_counts = defaultdict(lambda: defaultdict(int))
    
    for lesson in lessons:
        for pair in lesson.pairs:
            p_parts = re.split(r'[-\s]', pair.normalized_pinyin)
            h_chars = split_characters(pair.hanzi)
            
            if len(p_parts) == len(h_chars):
                for c, p in zip(h_chars, p_parts):
                    char_counts[c][p] += 1
    
    # è¿”å›æ¯ä¸ªå­—çš„ä¸»æµè¯»éŸ³
    return {c: max(ps.keys(), key=lambda x: ps[x]) 
            for c, ps in char_counts.items() if ps}

def analyze_ruby_pair(pinyin: str, hanzi: str, pron_db: Dict[str, str]) -> Optional[FixSuggestion]:
    """åˆ†æå•ä¸ª Ruby å¯¹æ˜¯å¦éœ€è¦ä¿®å¤"""
    p_parts = re.split(r'[-\s]', normalize_pinyin(pinyin))
    h_chars = split_characters(hanzi)
    
    original = f'#r("{pinyin}", "{hanzi}")'
    
    # æƒ…å†µ1: é•¿åº¦åŒ¹é…ï¼Œæ£€æŸ¥æ‹¼å†™é”™è¯¯
    if len(p_parts) == len(h_chars):
        for i, (char, py) in enumerate(zip(h_chars, p_parts)):
            if char in pron_db:
                expected = pron_db[char]
                sim = get_similarity(expected, py)
                if sim < 0.5 and sim > 0:
                    # å¯èƒ½æ˜¯æ‹¼å†™é”™è¯¯
                    new_parts = p_parts.copy()
                    new_parts[i] = expected
                    new_pinyin = "-".join(new_parts)
                    return FixSuggestion(
                        file="", line_num=0,
                        strategy=FixStrategy.REPLACE_PINYIN,
                        original=original,
                        problem=f"æ‹¼å†™é”™è¯¯: '{py}' åº”ä¸º '{expected}' (å­—: {char})",
                        suggestion=f'#r("{new_pinyin}", "{hanzi}")',
                        confidence=0.8
                    )
        return None  # åŒ¹é…è‰¯å¥½
    
    # æƒ…å†µ2: æ±‰å­—æ¯”æ‹¼éŸ³å¤š (æ¼å­—)
    if len(h_chars) > len(p_parts):
        diff = len(h_chars) - len(p_parts)
        # å°è¯•æ¨æ–­ç¼ºå¤±çš„æ‹¼éŸ³
        missing_chars = []
        inferred_pinyins = []
        
        for char in h_chars:
            if char in pron_db:
                inferred_pinyins.append((char, pron_db[char]))
            else:
                missing_chars.append(char)
                inferred_pinyins.append((char, None))
        
        if not missing_chars:
            # å¯ä»¥å®Œå…¨æ¨æ–­
            new_rubies = " ".join([f'#r("{py}", "{c}")' for c, py in inferred_pinyins])
            return FixSuggestion(
                file="", line_num=0,
                strategy=FixStrategy.SPLIT_RUBY,
                original=original,
                problem=f"æ±‰å­— ({len(h_chars)}å­—) > æ‹¼éŸ³ ({len(p_parts)}èŠ‚)",
                suggestion=new_rubies,
                confidence=0.7
            )
        else:
            # éœ€è¦ç”¨æˆ·è¾“å…¥
            return FixSuggestion(
                file="", line_num=0,
                strategy=FixStrategy.SPLIT_RUBY,
                original=original,
                problem=f"æ±‰å­— ({len(h_chars)}å­—) > æ‹¼éŸ³ ({len(p_parts)}èŠ‚), æ— æ³•æ¨æ–­ '{missing_chars[0]}'",
                suggestion="",
                confidence=0.0,
                needs_input=True,
                missing_char=missing_chars[0]
            )
    
    # æƒ…å†µ3: æ‹¼éŸ³æ¯”æ±‰å­—å¤š (å¤šå­—)
    if len(p_parts) > len(h_chars):
        return FixSuggestion(
            file="", line_num=0,
            strategy=FixStrategy.MERGE_RUBY,
            original=original,
            problem=f"æ‹¼éŸ³ ({len(p_parts)}èŠ‚) > æ±‰å­— ({len(h_chars)}å­—)",
            suggestion="",  # éœ€è¦äººå·¥åˆ¤æ–­
            confidence=0.0,
            needs_input=True
        )
    
    return None

def scan_file_for_fixes(lesson: LessonFile, pron_db: Dict[str, str]) -> List[FixSuggestion]:
    """æ‰«æå•ä¸ªæ–‡ä»¶ï¼Œè¿”å›æ‰€æœ‰ä¿®å¤å»ºè®®"""
    suggestions = []
    
    # ä½¿ç”¨æ­£åˆ™æ‰¾å‡ºæ‰€æœ‰ #r(...) å¹¶è®°å½•è¡Œå·
    lines = lesson.content.split('\n')
    pattern = r'#r\("([^"]+)",\s*"([^"]+)"\)'
    
    for line_num, line in enumerate(lines, 1):
        for match in re.finditer(pattern, line):
            pinyin, hanzi = match.group(1), match.group(2)
            fix = analyze_ruby_pair(pinyin, hanzi, pron_db)
            if fix:
                fix.file = lesson.filename
                fix.line_num = line_num
                suggestions.append(fix)
    
    return suggestions

def apply_fix(file_path: Path, suggestion: FixSuggestion, backup: bool = True) -> bool:
    """åº”ç”¨å•æ¡ä¿®å¤åˆ°æ–‡ä»¶"""
    if not suggestion.suggestion:
        return False
    
    content = file_path.read_text(encoding='utf-8')
    
    if suggestion.original not in content:
        return False
    
    if backup:
        backup_path = file_path.with_suffix(file_path.suffix + '.bak')
        if not backup_path.exists():
            shutil.copy(file_path, backup_path)
    
    new_content = content.replace(suggestion.original, suggestion.suggestion, 1)
    file_path.write_text(new_content, encoding='utf-8')
    return True

def run_fixer(
    lessons_dir: Path,
    target: Optional[str] = None,
    dry_run: bool = True,
    interactive: bool = False,
    auto: bool = False,
    backup: bool = True
):
    """ä¸»ä¿®å¤å…¥å£"""
    print("="*80)
    print("Ruby å¯¹ä¿®å¤å·¥å…·")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    lessons = load_lessons(lessons_dir)
    if not lessons:
        print("æœªæ‰¾åˆ°è¯¾ç¨‹æ–‡ä»¶")
        return
    
    # è¿‡æ»¤ç›®æ ‡
    if target and target != "--all":
        lessons = [l for l in lessons if target in l.filename]
        if not lessons:
            print(f"æœªæ‰¾åˆ°åŒ¹é… '{target}' çš„æ–‡ä»¶")
            return
    
    # å»ºç«‹å‘éŸ³æ•°æ®åº“
    all_lessons = load_lessons(lessons_dir)  # å…¨éƒ¨ç”¨äºå»ºåº“
    pron_db = build_pronunciation_db(all_lessons)
    print(f"å·²å»ºç«‹ {len(pron_db)} å­—å‘éŸ³æ•°æ®åº“")
    
    # æ‰«æé—®é¢˜
    all_fixes = []
    for lesson in lessons:
        fixes = scan_file_for_fixes(lesson, pron_db)
        all_fixes.extend(fixes)
    
    if not all_fixes:
        print("\nâœ… æœªå‘ç°éœ€è¦ä¿®å¤çš„é—®é¢˜")
        return
    
    print(f"\nå‘ç° {len(all_fixes)} å¤„å¾…ä¿®å¤é—®é¢˜")
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤º
    by_file = defaultdict(list)
    for fix in all_fixes:
        by_file[fix.file].append(fix)
    
    fixed_count = 0
    skipped_count = 0
    
    for filename, fixes in by_file.items():
        print(f"\nğŸ“„ {filename} ({len(fixes)} å¤„)")
        file_path = lessons_dir / filename
        
        for i, fix in enumerate(fixes, 1):
            print(f"\n  [{i}/{len(fixes)}] ç¬¬ {fix.line_num} è¡Œ")
            print(f"  åŸæ–‡: {fix.original}")
            print(f"  é—®é¢˜: {fix.problem}")
            
            if fix.suggestion:
                print(f"  å»ºè®®: {fix.suggestion}")
                print(f"  ç½®ä¿¡åº¦: {fix.confidence:.0%}")
            
            if dry_run:
                print("  [DRY-RUN] è·³è¿‡")
                continue
            
            if fix.needs_input:
                if interactive:
                    user_input = input(f"  è¾“å…¥ '{fix.missing_char}' çš„æ‹¼éŸ³ (ç•™ç©ºè·³è¿‡): ").strip()
                    if user_input:
                        # é‡æ–°æ„å»ºå»ºè®® (ç®€åŒ–å¤„ç†)
                        print(f"  â†’ éœ€æ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶æ·»åŠ : #r(\"{user_input}\", \"{fix.missing_char}\")")
                    skipped_count += 1
                else:
                    print("  [éœ€æ‰‹åŠ¨å¤„ç†]")
                    skipped_count += 1
                continue
            
            if interactive:
                choice = input("  åº”ç”¨ä¿®å¤? [y/n/s=è·³è¿‡æ–‡ä»¶/q=é€€å‡º] > ").strip().lower()
                if choice == 'q':
                    print("\nå·²é€€å‡º")
                    return
                if choice == 's':
                    print(f"  è·³è¿‡æ–‡ä»¶ {filename}")
                    break
                if choice != 'y':
                    skipped_count += 1
                    continue
            
            if auto or (interactive and choice == 'y'):
                if apply_fix(file_path, fix, backup):
                    print("  âœ“ å·²ä¿®å¤")
                    fixed_count += 1
                else:
                    print("  âœ— ä¿®å¤å¤±è´¥")
                    skipped_count += 1
    
    print("\n" + "="*80)
    print(f"å®Œæˆ! ä¿®å¤ {fixed_count} å¤„, è·³è¿‡ {skipped_count} å¤„")
    if dry_run:
        print("(å¹²è¿è¡Œæ¨¡å¼ï¼Œæœªå®é™…ä¿®æ”¹æ–‡ä»¶)")
